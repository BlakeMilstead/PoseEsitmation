{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3004991f-ba40-45fc-a293-02d3a820cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to C:\\Users\\dogat\\anaconda3\\Lib\\site-packages\\mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145136.csv ✅\n",
      "Processed Recording 2025-03-26 145136.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145435.csv ✅\n",
      "Processed Recording 2025-03-26 145435.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145629.csv ✅\n",
      "Processed Recording 2025-03-26 145629.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145706.csv ✅\n",
      "Processed Recording 2025-03-26 145706.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145757.csv ✅\n",
      "Processed Recording 2025-03-26 145757.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 145921.csv ✅\n",
      "Processed Recording 2025-03-26 145921.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 150012.csv ✅\n",
      "Processed Recording 2025-03-26 150012.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 150118.csv ✅\n",
      "Processed Recording 2025-03-26 150118.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 150221.csv ✅\n",
      "Processed Recording 2025-03-26 150221.mp4 ✅\n",
      "CSV saved: C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat_PoseCSVs\\squat_data_Recording 2025-03-26 150344.csv ✅\n",
      "Processed Recording 2025-03-26 150344.mp4 ✅\n",
      "✅ All videos processed!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "class poseDetector():\n",
    "    def __init__(self, mode=False, upBody=False, smooth=True, detectionCon=True, trackCon=0.5):\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(mode, upBody, smooth, detectionCon, trackCon)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks and draw:\n",
    "            self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, _ = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy, lm.x, lm.y, lm.z])\n",
    "        return lmList\n",
    "\n",
    "def process_squat_videos(video_folder: str):\n",
    "    output_folder = os.path.join(video_folder, \"Squat_PoseCSVs\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(video_folder):\n",
    "        if filename.lower().endswith(\".mp4\"):  \n",
    "            video_path = os.path.join(video_folder, filename)\n",
    "            save_squat_csv(video_path, output_folder)\n",
    "            print(f\"Processed {filename} ✅\")\n",
    "\n",
    "def save_squat_csv(video_path: str, output_folder: str):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't read {video_path}\")\n",
    "        return\n",
    "    \n",
    "    detector = poseDetector()\n",
    "    base_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    csv_file = os.path.join(output_folder, f\"squat_data_{base_filename}.csv\")\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Time\", \"Hip_Y\", \"Squat_State\"])\n",
    "\n",
    "        start_time = time.time()\n",
    "        hip_y_values = []\n",
    "\n",
    "        while True:\n",
    "            success, img = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            img = detector.findPose(img)\n",
    "            lmList = detector.findPosition(img)\n",
    "\n",
    "            timestamp = time.time() - start_time  \n",
    "\n",
    "            if lmList and len(lmList) > 24:  \n",
    "                hip_y = (lmList[23][2] + lmList[24][2]) / 2  \n",
    "                hip_y_values.append(hip_y)\n",
    "\n",
    "                if len(hip_y_values) > 30:\n",
    "                    min_hip = np.min(hip_y_values)\n",
    "                    max_hip = np.max(hip_y_values)\n",
    "                    squat_threshold = (max_hip + min_hip) / 2  \n",
    "\n",
    "                    squat_state = \"down\" if hip_y >= squat_threshold else \"up\"\n",
    "                else:\n",
    "                    squat_state = \"unknown\"  \n",
    "\n",
    "                writer.writerow([timestamp, hip_y, squat_state])\n",
    "                file.flush()  \n",
    "\n",
    "                cv2.putText(img, f\"Squat: {squat_state.upper()}\", (50, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Squat Detection\", img)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"CSV saved: {csv_file} ✅\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_folder = r\"C:\\Users\\dogat\\Desktop\\DL_Vidoes\"\n",
    "    process_squat_videos(video_folder)\n",
    "    print(\"✅ All videos processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b52204-d72b-4e7a-87e0-f759c65fc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e468f79-fbe7-4a57-9f60-5945de5fc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "video_path = r\"C:\\Users\\dogat\\Desktop\\DL_Vidoes\" # Update this path\n",
    "keypoints_data = []\n",
    "\n",
    "for file in os.listdir(video_path):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        cap = cv2.VideoCapture(os.path.join(video_path, file))\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                keypoints = [ \n",
    "                    (lm.x, lm.y, lm.z) for lm in results.pose_landmarks.landmark\n",
    "                ]\n",
    "                keypoints_data.append(keypoints)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "keypoints_data = np.array(keypoints_data)\n",
    "np.save(\"pose_data.npy\", keypoints_data)  # Save extracted features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d4e273-0e1a-49c9-8c88-2cf56493e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_squat(landmarks):\n",
    "    hip_y = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value][1]  # Y-coordinate of the left hip\n",
    "    return \"DOWN\" if hip_y > 0.6 else \"UP\"  # Adjust threshold based on video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a425ccf-041d-4790-bc59-bf8639fb0c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dogat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.7106 - loss: 0.5948\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8613 - loss: 0.3657\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8508 - loss: 0.3462\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8779 - loss: 0.2851\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8913 - loss: 0.2658\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8813 - loss: 0.2662\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9171 - loss: 0.2207\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8978 - loss: 0.2475\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9037 - loss: 0.2335\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9023 - loss: 0.2348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "X_train = np.load(\"pose_data.npy\")  # Load extracted features\n",
    "y_train = np.array([label_squat(frame) for frame in X_train])  # Convert labels to array\n",
    "\n",
    "# Convert labels to numeric values (0 = DOWN, 1 = UP)\n",
    "y_train = np.array([0 if label == \"DOWN\" else 1 for label in y_train])\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(33, 3)),  # 33 keypoints with (x, y, z)\n",
    "    LSTM(32),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  # Binary classification (UP or DOWN)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"squat_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28f0f19-4d47-414f-8e3c-75aaacf28655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_frame(frame, model, pose, squat_threshold=0.5):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Extract all 33 keypoints (x, y, z)\n",
    "        keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)  # Reshape to (1, 33, 3)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(keypoints)[0][0]\n",
    "        print(f\"Model prediction: {prediction}, Threshold: {squat_threshold}\")\n",
    "\n",
    "        label = \"UP\" if prediction > squat_threshold else \"DOWN\"\n",
    "    else:\n",
    "        label = \"UNKNOWN\"\n",
    "\n",
    "    return label, results.pose_landmarks\n",
    "\n",
    "\n",
    "# Run on live video\n",
    "cap = cv2.VideoCapture(0)  # Webcam\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    label = classify_frame(frame)\n",
    "    cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Squat Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdc2f65-0184-4bc1-a73c-541fa4ed9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path to your video\n",
    "video_path = r\"C:\\\\Users\\\\dogat\\\\Desktop\\\\DL_Vidoes\"  # Update with your path\n",
    "\n",
    "# Open a sample video to extract frames\n",
    "cap = cv2.VideoCapture(os.path.join(video_path, r\"C:\\Users\\dogat\\Downloads\\Squat1.MOV\"))  # Change file name\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB (MediaPipe works with RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame to get pose landmarks\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # If landmarks are found, draw them on the frame\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame with pose landmarks\n",
    "    cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "    # Break the loop if the user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf5a473-69aa-4d02-a610-6af44c9478d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hip Y-coordinate: 0.5467966198921204\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 1), dtype=float32). Expected shape (None, 33, 3), but input has incompatible shape (1, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Get classification label and pose landmarks\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m label, landmarks \u001b[38;5;241m=\u001b[39m classify_frame(frame, model, pose)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# If landmarks exist, draw them\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m landmarks:\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mclassify_frame\u001b[1;34m(frame, model, pose, squat_threshold)\u001b[0m\n\u001b[0;32m     12\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[hip_y]])  \u001b[38;5;66;03m# Model expects single feature input\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(keypoints)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msquat_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m squat_threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOWN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 1), dtype=float32). Expected shape (None, 33, 3), but input has incompatible shape (1, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "def classify_frame(frame, model, pose, squat_threshold=0.5):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Extract keypoints (hip y-coordinate for squat detection)\n",
    "        hip_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].y\n",
    "        \n",
    "        # Print debug information\n",
    "        print(f\"Hip Y-coordinate: {hip_y}\")\n",
    "        \n",
    "        keypoints = np.array([[hip_y]])  # Model expects single feature input\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(keypoints)[0][0]\n",
    "        print(f\"Model prediction: {prediction}, Threshold: {squat_threshold}\")\n",
    "\n",
    "        label = \"UP\" if prediction > squat_threshold else \"DOWN\"\n",
    "    else:\n",
    "        label = \"UNKNOWN\"\n",
    "\n",
    "    return label, results.pose_landmarks\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(\"squat_classifier.h5\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(os.path.join(video_path, r\"C:\\Users\\dogat\\Desktop\\DL_Vidoes\\Squat1.MOV\"))  # Change file name\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get classification label and pose landmarks\n",
    "    label, landmarks = classify_frame(frame, model, pose)\n",
    "\n",
    "    # If landmarks exist, draw them\n",
    "    if landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Overlay label on frame\n",
    "    cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with keypoints and label\n",
    "    cv2.imshow(\"Pose Estimation with Label\", frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd5325c-77bf-4193-9bf6-713b2c375c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step\n",
      "Model prediction: 0.9948984980583191, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Model prediction: 0.9931038022041321, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Model prediction: 0.9922636151313782, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Model prediction: 0.9914385676383972, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Model prediction: 0.9926180839538574, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Model prediction: 0.9924228191375732, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Model prediction: 0.9923264980316162, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Model prediction: 0.9926060438156128, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Model prediction: 0.9925346970558167, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Model prediction: 0.9925343990325928, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Model prediction: 0.9926338195800781, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Model prediction: 0.9926160573959351, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Model prediction: 0.9931078553199768, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Model prediction: 0.9932153820991516, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Model prediction: 0.9934352040290833, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Model prediction: 0.9934210777282715, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9921138286590576, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9920353889465332, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9910956025123596, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Model prediction: 0.9916232228279114, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Model prediction: 0.9921258687973022, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Model prediction: 0.9915022253990173, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Model prediction: 0.9911670684814453, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9907077550888062, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Model prediction: 0.9837974309921265, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9754918217658997, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Model prediction: 0.9869422912597656, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Model prediction: 0.9869514107704163, Threshold: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Model prediction: 0.9891350269317627, Threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class poseDetector():\n",
    "    def __init__(self, mode=False, upBody=False, smooth=True, detectionCon=True, trackCon=0.5):\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(mode, upBody, smooth, detectionCon, trackCon)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)  # Use the pose object to process the frame\n",
    "        if self.results.pose_landmarks and draw:\n",
    "            self.mpDraw.draw_landmarks(img, self.results.pose_landmarks, self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, _ = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy, lm.x, lm.y, lm.z])\n",
    "        return lmList\n",
    "\n",
    "def classify_frame(frame, model, pose, squat_threshold=0.5):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.pose.process(frame_rgb)  # Fix here: Access `pose.process`\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Extract all 33 keypoints (x, y, z)\n",
    "        keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)  # Reshape to (1, 33, 3)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(keypoints)[0][0]\n",
    "        print(f\"Model prediction: {prediction}, Threshold: {squat_threshold}\")\n",
    "\n",
    "        label = \"UP\" if prediction > squat_threshold else \"DOWN\"\n",
    "    else:\n",
    "        label = \"UNKNOWN\"\n",
    "\n",
    "    return label, results.pose_landmarks\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(\"squat_classifier.h5\")\n",
    "\n",
    "# Open the video\n",
    "video_path = r\"C:\\Users\\dogat\\Desktop\\DL_Vidoes\\fortSquat1.mov\"  # Change file name if needed\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize pose detector\n",
    "pose = poseDetector()\n",
    "\n",
    "# Initialize frame predictions for smoothing (e.g., last 5 frames)\n",
    "frame_predictions = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get classification label and pose landmarks\n",
    "    label, landmarks = classify_frame(frame, model, pose)\n",
    "\n",
    "    # Append the current prediction (1 for \"UP\", 0 for \"DOWN\") to the list\n",
    "    frame_predictions.append(1 if label == \"UP\" else 0)\n",
    "\n",
    "    # Keep only the last 5 predictions (e.g., smoothing over 5 frames)\n",
    "    if len(frame_predictions) > 5:\n",
    "        frame_predictions.pop(0)\n",
    "\n",
    "    # Apply majority voting or moving average for smoothing\n",
    "    if np.mean(frame_predictions) > 0.5:\n",
    "        final_label = \"UP\"\n",
    "    else:\n",
    "        final_label = \"DOWN\"\n",
    "\n",
    "    # If landmarks exist, draw them\n",
    "    if landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, landmarks, mp.solutions.pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Overlay final label on frame\n",
    "    cv2.putText(frame, final_label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with keypoints and label\n",
    "    cv2.imshow(\"Pose Estimation with Label\", frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bbe71-25cb-4ead-8ba2-82c692d21816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7effc8-6633-49bd-af95-2e39d01d4938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
